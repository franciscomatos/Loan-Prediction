# -*- coding: utf-8 -*-
"""Machine Learning Projects For Beginners

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Zmgm-4T4ttobCCalDpQKgtHK-sIxItyR

# Loan Prediction

This dataset introduces some complications since not all features are numeric and there are some missing values.
"""

import pandas as pd

loan_train_data = pd.read_csv('/Users/franciscomatos/Desktop/loan_train.csv', index_col='Loan_ID', sep=',', header=0)
loan_test_data = pd.read_csv('/Users/franciscomatos/Desktop/loan_test.csv', index_col='Loan_ID', sep=',', header=0)
# check for features type
print(loan_train_data.dtypes)

# first let's check for missing values 

print(loan_train_data[loan_train_data.isnull().any(axis=1)])

# 134 rows with missing values

"""For the numerical features we will fill the missing values with the mean in order to not change the distribution. For categorical features we will use the most common."""

import numpy as np
from sklearn.impute import SimpleImputer

numerical_features = loan_train_data.select_dtypes(include='number')
categorical_features = loan_train_data.select_dtypes(include='object')

numerical_imputer = SimpleImputer(strategy='mean', missing_values=np.nan, copy=True)
categorical_imputer = SimpleImputer(strategy='most_frequent', missing_values=np.nan, copy=True)

filled_numerical = pd.DataFrame(numerical_imputer.fit_transform(numerical_features), 
                     columns=numerical_features.columns)
filled_categorical = pd.DataFrame(categorical_imputer.fit_transform(categorical_features), 
                     columns=categorical_features.columns)

filled_data = filled_numerical.join(filled_categorical, how='right')

filled_data.describe()

"""In order to apply linear regression we need to dummify the categorical values and apply normalization. Binary variables should be maintained."""

from sklearn.preprocessing import MinMaxScaler, OneHotEncoder


# normalization
transf = MinMaxScaler(feature_range=(0, 1), copy=True).fit(filled_numerical)
normalized_numerical = pd.DataFrame(transf.transform(filled_numerical), 
                                    columns= filled_numerical.columns)

normalized_data = normalized_numerical.join(filled_categorical, how='right')
normalized_data.describe(include='all')

# dummification

# get all binary features and its values
features_count = filled_categorical.nunique()
# series containg the number of unique values in each column
bin = features_count[features_count==2].index
# grouping in list of list in the format [feature name, option1, option2]
binary_features = []
binary_features_names = []
for feat in bin:
  feature = [feat]
  for value in filled_categorical[feat].unique():
    feature += [value]
  binary_features += [feature]
  binary_features_names += [feat]


# map the options into binary values
den_data = normalized_data[binary_features_names].copy()
for [feature, option1, option2] in binary_features:
  den_data[feature] = normalized_data[feature].map({option1:1, option2:0})
  filled_categorical.pop(feature)

den_data.describe(include='all')

# dummify the rest
def dummify(final_data, categorical_data, cols_to_dummify):
  # encoder initialization
  one_hot_encoder = OneHotEncoder(sparse=False)

  for var in cols_to_dummify:
    one_hot_encoder.fit(categorical_data[var].values.reshape(-1, 1))
    # each value will correspond to one feature
    feature_names = one_hot_encoder.get_feature_names([var])
    transformed_data = one_hot_encoder.transform(categorical_data[var].values.reshape(-1, 1))
    final_data = pd.concat((final_data, pd.DataFrame(transformed_data, columns=feature_names)), 1)

  return final_data

final_data = dummify(normalized_numerical, filled_categorical, filled_categorical.columns)
final_data = pd.concat((final_data, den_data), 1)
final_data.describe(include='all')

"""We can finally train our linear model."""

from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
import sklearn.metrics as metrics

y = final_data.pop('Loan_Status').values
X = final_data.values

trnX, tstX, trnY, tstY = train_test_split(X, y, train_size=0.7, stratify=y)

classifiers = ['KNN', 'Naive Bayes', 'Decision Tree (ent)', 'Decision Tree (gini)', 
               'Random Forests (sqrt)', 'Random Forests (log2)', 'SVM']
final_acc = []

"""Let's compare several classification models. First, let's start with KNN."""

import matplotlib.pyplot as plt
import numpy as np

xvalues = []
yvalues = []
for i in range(1,40, 2):
  xvalues.append(i)
  knn = KNeighborsClassifier(n_neighbors=i, metric='euclidean')
  knn.fit(trnX, trnY)
  prdY = knn.predict(tstX)
  yvalues.append(metrics.accuracy_score(tstY, prdY))

plt.figure()
plt.plot(xvalues, yvalues)
plt.xlabel("N")
plt.ylabel("Accuracy")
plt.suptitle("KNN parameter analysis")
plt.show()

acc = np.array(yvalues)
m = np.argmax(acc)
final_acc.append(yvalues[m])
print("N with the best accuracy:", xvalues[m], "acc:", yvalues[m])

"""Now let's apply Naive Bayes."""

from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB

estimators = {'GaussianNB': GaussianNB(), 
              'MultinomialNB': MultinomialNB(), 
              'BernoulyNB': BernoulliNB()}

xvalues = []
yvalues = []
for label, classifier in estimators.items():
    xvalues.append(label)
    classifier.fit(trnX, trnY)
    prdY = classifier.predict(tstX)
    yvalues.append(metrics.accuracy_score(tstY, prdY))

plt.figure()
ax = plt.gca()
ax.set_title('Comparison of Naive Bayes Models')
ax.set_xlabel('Estimators')
ax.set_ylabel('Accuracy')
ax.set_ylim(0.0, 1.0)
ax.set_xticklabels(xvalues, rotation=90, fontsize='small')
ax.bar(xvalues, yvalues, edgecolor='grey')
plt.show()

nb_max = np.argmax(yvalues)
final_acc.append(yvalues[nb_max])
print("max acc for naive bayes:", yvalues[nb_max], "through", xvalues[nb_max], "estimator")

"""Now let's try with decision trees."""

from sklearn.tree import DecisionTreeClassifier

min_samples_leaf = [.001, .0025, .005, .0075, .01, .025, .05]
max_depths = [5, 10, 25, 50]
criteria = ['entropy', 'gini']

def multiple_line_chart(ax: plt.Axes, xvalues: list, yvalues: dict, title: str, xlabel: str, ylabel: str, percentage=False):
    legend: list = []
    ax.set_title(title)
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)
    if percentage:
        ax.set_ylim(0.0, 1.0)

    for name, y in yvalues.items():
        ax.plot(xvalues, y)
        legend.append(name)
    ax.legend(legend, loc='best', fancybox = True, shadow = True)

plt.figure()
fig, axs = plt.subplots(1, 2, figsize=(16, 4), squeeze=False)
v = []
for k in range(len(criteria)):
    f = criteria[k]
    values = {}
    for d in max_depths:
      yvalues = []
      for s in min_samples_leaf:
        tree = DecisionTreeClassifier(min_samples_leaf=s, max_depth=d, criterion=f)
        tree.fit(trnX, trnY)
        prdY = tree.predict(tstX)
        yvalues.append(metrics.accuracy_score(tstY, prdY))
      values[d] = yvalues
    multiple_line_chart(axs[0,k], min_samples_leaf, values, f, "min_samples_leaf",
                        "accuracy", True)
    v.append(values)
    
plt.show()
#max_values_entropy = []
for i in range(len(v)):
  max_acc = []
  max_index_samples = []
  for depth, values in v[i].items():
    max_i_samples = np.argmax(values)
    max_acc.append(v[i][depth][max_i_samples])
    max_index_samples.append(max_i_samples)

  # max depth that maximizes accuracy
  # if there are equal values for different depths, it will select the lowest depth
  max_i_depth = np.argmax(max_acc)

  print("max acc for", criteria[i] ,":", max_acc[max_i_depth], "max_depth =", 
        max_depths[max_i_depth],"min_samples_leaf =", min_samples_leaf[max_index_samples[max_i_depth]])

  final_acc.append(max_acc[max_i_depth])

"""Let's check if we can get even better results with random forests."""

from sklearn.ensemble import RandomForestClassifier

n_estimators = [5, 10, 25, 50, 75, 100, 150, 200, 250, 300, 350, 400]
max_depths = [5, 10, 25, 50]
max_features = ['sqrt', 'log2']


plt.figure()
fig, axs = plt.subplots(1, 2, figsize=(10, 4), squeeze=False)
v = []
for k in range(len(max_features)):
  f = max_features[k]
  values = {}
  for d in max_depths:
    yvalues = []
    for e in n_estimators:
      rf = RandomForestClassifier(n_estimators=e, max_depth=d, max_features=f)
      rf.fit(trnX, trnY)
      prdY = rf.predict(tstX)
      yvalues.append(metrics.accuracy_score(tstY, prdY))
    values[d] = yvalues
  v.append(values)
  multiple_line_chart(axs[0, k], n_estimators, values, 'Random Forests with %s features'%f, 'nr estimators', 
                             'accuracy', percentage=True)
    
plt.show()

for i in range(len(v)):
  max_acc = []
  max_index_samples = []
  for depth, values in v[i].items():
    max_i_samples = np.argmax(values)
    max_acc.append(v[i][depth][max_i_samples])
    max_index_samples.append(max_i_samples)

  # max depth that maximizes accuracy
  # if there are equal values for different depths, it will select the lowest depth
  max_i_depth = np.argmax(max_acc)

  print("max acc for", max_features[i] ,":", max_acc[max_i_depth], "max_depth =", 
        max_depths[max_i_depth],"n_estimators =", n_estimators[max_index_samples[max_i_depth]])
  
  final_acc.append(max_acc[max_i_depth])

"""Let's now try with SVM with various kernels."""

from sklearn import svm

svm_kernels = ['linear', 'poly', 'gaussian', 'sigmoid']
svm_acc = []
clf = svm.LinearSVC()
clf.fit(trnX, trnY)
prdY = clf.predict(tstX)
acc = metrics.accuracy_score(tstY, prdY)
svm_acc.append(acc)

print("acc using SVM with a linear kernel:", acc)

yvalues = []
xvalues = []
for i in range(1,10):
  xvalues.append(i)
  clf = svm.SVC(kernel='poly', degree=i)
  clf.fit(trnX, trnY)
  prdY = clf.predict(tstX)
  acc = metrics.accuracy_score(tstY, prdY)
  yvalues.append(acc)

svm_acc.append(np.max(yvalues))

plt.figure()
plt.plot(xvalues, yvalues)
plt.xlabel('Degree')
plt.ylabel('Accuracy')
plt.suptitle('Polynomial Kernel Degree Evaluation')
plt.show()

poly_max = np.argmax(yvalues)
print("acc using SVM with polynomial kernel:", yvalues[poly_max], "degree:", xvalues[poly_max])


g_clf = svm.SVC(kernel='rbf')
g_clf.fit(trnX, trnY)
prdY = g_clf.predict(tstX)
acc = metrics.accuracy_score(tstY, prdY)
svm_acc.append(acc)

print("acc using SVM with a Gaussian kernel:", acc)

s_clf = svm.SVC(kernel='sigmoid')
s_clf.fit(trnX, trnY)
prdY = s_clf.predict(tstX)
acc = metrics.accuracy_score(tstY, prdY)
svm_acc.append(acc)

print("acc using SVM with a sigmoid kernel:", acc)

final_acc.append(np.max(svm_acc))

"""Now let's compare all the classifiers and see what works best for this dataset."""

plt.figure()
ax = plt.gca()
ax.set_title('Comparison of Different Classifiers')
ax.set_xlabel('Classifiers')
ax.set_ylabel('Accuracy')
ax.set_ylim(0.7, 0.85)
ax.set_xticklabels(classifiers, rotation=90, fontsize='small')
ax.bar(classifiers, final_acc, edgecolor='grey')
plt.show()

"""As expected, the classifiers that performed the best were Random Forests and SVM."""